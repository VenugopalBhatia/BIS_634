{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Ex1' from '/Users/venugopalbhatia/Documents/Computational Methods for Informatics/Assignments/HW_Nov16/Ex1.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import Ex1\n",
    "import importlib\n",
    "importlib.reload(Ex1)  \n",
    "### These arent really required but python tends to cache the imports \n",
    "# and if one makes changes to the file, the reload is required to get the updated methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For multivariate gradient descent we'd want to take the partial derivative and do a simulataneous update to all params.\n",
    "#### Since we can't calculate partial derivative, I'd take f(a+h,b) - f(a-h,b)/2h to be the partial derivative with respect to a keeping b constant and vice versa\n",
    "#### Keeping this simple for the purposes of our assignment in the sense that the loop is broken if both the derivatives go to approximately 0, I could alternatively look at a, b and t1, t2 and determine that if a, b dont change much we can stop. A third case would be keeping track of the changes to the function value but that would mean more calls to the api. Thus keeping track of the gradient even though not technically correct where multiple points of minima exist made sense to me in this context and I tried implementing that strategy.\n",
    "#### There is an edge case where a or b may go beyond 1 or below 0, in that case reset the for loop and sample new a and b.\n",
    "#### There is also a check variable which would return false if our function went through the n_iterations without the slope going to 0. That would probably mean it didn't converge in those iterations.\n",
    "#### To find the points of extrema, I ran this 5 times with randomly chosen a and b parallely so different start points may result in convergence at different extrema points. Alternatively I could implement something like Adam's optimization or a momentum based gradient which would increase likelihood of convergence at global minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cpus = mp.cpu_count()\n",
    "pool = mp.Pool(num_cpus)\n",
    "params = pool.starmap(Ex1.gradientDescent,[(seed,0.1,1000,0.001,0.001) for seed in range(5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0.21641200292731222, 0.6890324213723863), True),\n",
       " ((0.2164860412025592, 0.6890757384421452), True),\n",
       " ((0.7119276471419997, 0.1689624938278761), True),\n",
       " ((0.2164144595746229, 0.6890237036180802), True),\n",
       " ((0.7120668550136922, 0.16909915167574485), True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It seems 2 major value pairs for a, b exist:\n",
    " (0.2164,0.6890) and (0.7120,0.1690)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I think generally running the gradient descent repeatedly from different random start points and seeing if convergence is acheived at the same parameter values or not should take care of multiple extrema points. Alternatively if we get stuck at a point of extrema we could implement something like adding a random value to the parameters a and b in order for them to escape that point and then see convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10000017234\n",
      "1.0000000429\n"
     ]
    }
   ],
   "source": [
    "for _params in params[-2:]:\n",
    "    print(Ex1.f(_params[0][0],_params[0][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
