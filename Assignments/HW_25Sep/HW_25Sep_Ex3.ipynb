{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Here what's probably gone wrong is that the user loaded the entire file into memory and then is iterating through it line by line and appending weights to a list.\n",
    "\n",
    "#### Possible remedies:\n",
    "\n",
    "* One possible issue could be that 32-bit python has access to about just 4 GB data at a time, so switching over to 64-bit could help\n",
    "* The user seems to be using lists, switching over to np arrays with dtype float could help bring down the overhead associated with storing float values in a list, it could possibly more than halve the memory being taken.\n",
    "* Rather than take average of the entire set of weights, since we have to sum the weights anyway, maintain a sum variable where sum+=weight, and counter variable to keep count of weights.\n",
    "So Something like:\\\n",
    "    with open('weights.txt') as f:\\\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; sum_ = 0\\\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; count_ = 0\\\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp; for line in f:\\\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sum_+=float(line)\\\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; count_+=1\\\n",
    "    print(\"average =\", sum_ / count_)\n",
    "* The file could be read in chunks, average of the chunk calculated and written to a file and ultimately we take average of the calculated means of the chunks."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}